\clearpage


\section[W25: The 3rd workshop on Towards Knowledgeable Foundation Models]{Workshop 25}

\begin{center}
    {\Large \textbf{W25: The 3rd workshop on Towards Knowledgeable Foundation Models}}

Yuji Zhang, Xiaozhi Wang, Mor Geva, Chi Han, Shangbin Feng, Silin Gao, Sha Li, Manling Li, Heng Ji

    Friday, August 1, 2025

 Level 1 Hall 1.14
    
\end{center}

Description: Knowledge has been an important pre-requisite for a variety of AI applications, and is typically sourced from either structured knowledge sources such as knowledge bases and dictionaries or unstructured knowledge sources such as Wikipedia documents. More recently, researchers have discovered that language models already possess a significant amount of knowledge through pre-training: LLMs can be used to generate commonsense knowledge and factual knowledge context for question answering. While the results are encouraging, there are still lingering questions: Where does this knowledge come from? How much do language models know? Is this knowledge reliable? If some knowledge is wrong, can we fix it?

