\clearpage


\section[W05: Workshop on Meaningful, Efficient, and Robust Evaluation of LLMs and GEM: Natural Language Generation, Evaluation, and Metrics]{Workshop 05}

\begin{center}
    {\Large \textbf{W05: Workshop on Meaningful, Efficient, and Robust Evaluation of LLMs and GEM: Natural Language Generation, Evaluation, and Metrics}}

    Sebastian Gehrmann, Gabriel Stanovsky, Enrico Santus, Itay Itzhak, Jo√£o Sedoc, Kaustubh Dhole, Michal Shmueli Scheuer, Miruna Clinciu, Ofir Arviv, Rotem Dror, Simon Mille, Yotam Perlitz, Oyvind Tafjord

    Thursday, July 31
    
    Level 2 Hall C

\end{center}

Description: Evaluating large language models (LLMs) is challenging. Running LLMs over medium or large scale corpus can be prohibitively expensive; they are consistently shown to be highly sensitive to prompt phrasing, and it is hard to formulate metrics which differentiate and rank different LLMs in a meaningful way. Consequently, the validity of the results obtained over popular benchmarks such as HELM or MMLU, lead to brittle conclusions (Sclar er al., 2024, Mizrahi et al., 2024, Alzahrani et al., 2024). We believe that meaningful, efficient, and robust evaluation is one of the cornerstones of the scientific method, and that achieving it should be a community-wide goal.

In this workshop we seek innovative research relating to the evaluation of LLMs and language generation systems in general. This includes, but is not limited to, robust, reproducible and efficient evaluation metrics, as well as new approaches for collecting evaluation data which can help in better differentiating between different systems and understanding their current bottlenecks.


