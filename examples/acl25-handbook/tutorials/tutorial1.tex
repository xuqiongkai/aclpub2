\begin{center}
    \Large{\textbf{"T1: Inverse Reinforcement Learning Meets Large Language Model Alignment"}\\}
    \par\bigskip
    \large{Cutting-edge}\\
    \large{Mihaela van der Schaar and Hao Sun}\\
    \par\bigskip

\end{center}

Large Language Model (LLM) alignment remains one of the most critical challenges in reinforcement learning. As the success of models like DeepSeek-R1 demonstrates, improving alignment requires better architectures and a deeper understanding of reinforcement learning (RL) and reward modeling. This tutorial explores the connection between Inverse Reinforcement Learning (IRL) and LLM alignment, offering a structured roadmap for researchers and practitioners.

We frame LLM alignment as an inverse RL problem, contrasting traditional reinforcement learning with inverse methods that infer rewards from human data. A key focus is on reward models, examining how they are constructed from various data sources, including mathematical reasoning, binary feedback, preference data, and demonstrations.

Beyond theory, we delve into infrastructure and practical implementation, showcasing how to efficiently evaluate IRL-based LLM alignment ideas in minutes. We conclude with insights from sparse-reward RL, covering reward shaping, credit assignment, and lessons from self-play.

By the end of this tutorial, attendees will gain a practical and theoretical understanding of LLM alignment through inverse RL, equipping them with the tools to build better-aligned models efficiently.

\begin{center}
    \noindent\rule{200px}{1pt}
\end{center}
